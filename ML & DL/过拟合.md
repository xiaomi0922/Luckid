> ###### 摘要
> 过拟合的问题在机器学习算法中很常见，在深度学习中更常见的是模型train不起来，而不是过拟合，过拟合的处理方法可以是：降低模型复杂度，正则，dropout，earlystop，数据增强，交叉验证等。
### 1、正则
正则是防止过拟合的方法，分为L1正则和L2正则

L1正则

>$argmin J(\theta)=\frac{1}{2}\sum_{i=1}^m (y_i - \omega^Tx_i)^2 + \lambda||\omega||_1$ 

L2正则

>$argmin J(\theta)=\frac{1}{2}\sum_{i=1}^m (y_i - \omega^Tx_i)^2 + \lambda||\omega||_2^2$ 

从结构化风险最小化的角度来看，L1和L2正则都是使得误差最小化，同时模型越简单越好。
下图中等高线为不带正则的原函数L，我们之前目标是找到最内侧的点，这时误差最小，但是加入了正则后，变成了损失函数原函数与正则项的和，此时再取最内侧的圆心处的点时误差和可能会很大，因此最有可能的方案是取折中的位置，同时可以看出在登高线一圈的位置，只有与正则项相切时，才能保证二者的和最小，所以损失函数的解，一定是菱形或者圆形与某条等高线的切点位置。

![](https://upload-images.jianshu.io/upload_images/18416757-1067e0f539ccaab3.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
同时L1正则更容易相交于坐标轴，当特征多维时一些特征将会使0，所以会有稀疏的特性，即特征选择；而L2正则会使得切点接近坐标轴，但不至于为0，因此得到的都是较小的平滑的值，具体为什么会相交于坐标轴参见[知乎](https://zhuanlan.zhihu.com/p/35356992)。

> L1和L2的使用场景：如果单纯的是想让参数w变小的场景，建议优先使用L2 Norm，在已知参数存在很多0值的时候，即存在的解释稀疏的，建议使用L1 Norm，具有变量选择的功能；也可以L1和L2结合起来使用，如果是线性回归模型，同时加入了L1和L2正则，模型就变成了Elastic Net；


> 不加正则时得到的参数$\omega_1$，加入正则得到的参数$\omega_2$，则$f(\omega_1)<=f(\omega_2)$，因为加入正则后意味着参数空间变小，能得到最优解的概率降低，拟合程度变差，多以Loss比没加正则时大。
```
（这个得仔细看看）L1正则先验分布是Laplace分布，L2正则先验分布是Gaussian分布
```
### 2、Dropout
### 3、bagging
### 4、early_stop
### 5、交叉验证



参考：
[https://zhuanlan.zhihu.com/p/35356992](https://zhuanlan.zhihu.com/p/35356992)